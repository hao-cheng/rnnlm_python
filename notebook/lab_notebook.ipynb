{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Your first neuron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Unit\n",
    "Let's define the following recurrent unit\n",
    "$$ h_t = f(W_{hh} h_{t-1} + W_{hx}x_t) $$\n",
    "where $$t=1,\\ldots,\\infty,h_t \\text{ is the current state}, h_{t-1} \\text{ is the previous state}, $$\n",
    "$$x_t \\text{ is the current input}.$$\n",
    "Here, f(y) is the sigmoid function, $$f(y)=\\frac{1}{1+\\exp(-y)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the sigmoid function look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDo5Q8zsKWeIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f422abdf5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot sigmoid function in 1D\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(y):\n",
    "    return 1 / (1 + np.exp(-y))\n",
    "\n",
    "# You can change the bound from 5, 10, 20 up to 50 to see the shape change\n",
    "bound = 5\n",
    "x = np.arange(-bound, bound, 0.5)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of matrix A is 3x5\n",
      "The size of vector b is 5x1\n",
      "The size of vector c is 3x1\n",
      "The vector c is\n",
      "[[ 5]\n",
      " [ 5]\n",
      " [ 5]]\n"
     ]
    }
   ],
   "source": [
    "## How to code algebracially?\n",
    "\n",
    "## Matrix-vector multiplication\n",
    "## A is a nxt maxtrix, b is a tx1 vector\n",
    "## c is a nx1 vector, the multiplication result of A and b\n",
    "n = 3\n",
    "t = 5\n",
    "A = np.ones([n, t])   # initlize A as a nxt all one matrix\n",
    "print 'The size of matrix A is {}x{}'.format(A.shape[0], A.shape[1])\n",
    "b = np.ones([t, 1]) # initlize b as a tx1 all one vector\n",
    "print 'The size of vector b is {}x{}'.format(b.shape[0], b.shape[1])\n",
    "c = np.dot(A, b)\n",
    "print 'The size of vector c is {}x{}'.format(c.shape[0], c.shape[1])\n",
    "\n",
    "print 'The vector c is'_\n",
    "print c.astype('i')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up the forward function of the recurrent unit.\n",
    "Please fill in < input your code here > below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Given input x, previous state hprev, parameter matrices Whx and Whh\n",
    "## Compute the current state h\n",
    "## Code up the forward function of the recurrent unit define above\n",
    "## Remember to return the current state h\n",
    "\n",
    "hidden_size = 10\n",
    "input_size = 5\n",
    "\n",
    "Whh = np.zeros([hidden_size, hidden_size])\n",
    "Whh += np.random.uniform(-0.1, 0.1, [hidden_size, hidden_size])\n",
    "\n",
    "Whx = np.zeros([hidden_size, input_size])\n",
    "Whx += np.random.uniform(-0.1, 0.1, [hidden_size, input_size])\n",
    "\n",
    "def forward_function(x, hprev, Whx, Whh):\n",
    "    ## first compute the matrix-vector multiplication\n",
    "    # Whx x + Whh hprev\n",
    "    ##h = <input your code here>\n",
    "    h = np.dot(Whx, x) + np.dot(Whh, hprev)\n",
    "    ## use the sigmoid function to compute the current state\n",
    "    #h = sigmoid(<input your code here>)\n",
    "    h = sigmoid(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "x = np.random.randn(input_size, 1)\n",
    "hprev = np.zeros([hidden_size, 1])\n",
    "h = forward_function(x, hprev, Whx, Whh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## perform assert here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An import component of making the neuron to learn is to train it through gradient optimization. In order to do that, we first have to learn to take the derivative of the above unit.\n",
    "Let's do the derivatives in matrix form!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the cheatsheet:\n",
    "\n",
    "First let us define\n",
    "$$ y = W_{hh} h_{t-1} + W_{hx} x_t.$$\n",
    "### Important gradient one: \n",
    "\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = f\\prime(y)^T W_{hh},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f\\prime(y)=\\frac{\\partial f(y)}{y}=\\frac{\\partial \\frac{1}{1+\\exp(-y)}}{\\partial y}=\\frac{\\exp(-y)}{(1+\\exp(-y))^2} = (1-f(y))f(y),\n",
    "$$ is the gradient of sigmoid function.\n",
    "Can you verfiy the above gradient?\n",
    "\n",
    "### Important gradient two:\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial W_{hh}} = \\frac{\\partial h_t}{\\partial y} h_{t-1}^T = f\\prime(y) h_{t-1}^T\n",
    "$$\n",
    "\n",
    "### Important gradient three:\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial W_{hx}} = \\frac{\\partial h_t}{\\partial y} x_{t}^T = f\\prime(y) x_{t}^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up a backward function which accepts a training/error signal to weight the gradients and output the three gradients above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Given a training/error signal dEdh, input x, previous state hprev\n",
    "## parameters Whh, Whx\n",
    "## Return the three graidents above\n",
    "\n",
    "def backward_function(x, hprev, dEdh, Whx, Whh):\n",
    "    ## compute the gradient of sigmoid function\n",
    "    #f_prime = <input your code here>\n",
    "    f_prime = (1 - h) * h\n",
    "    ## weight the gradient by training/error signal\n",
    "    f_prime *= dEdh\n",
    "    ## compute the gradient one \n",
    "    #dEdhprev = <input your code here>\n",
    "    dEdhprev = np.dot(f_prime.T, Whh)\n",
    "    ## compute the gradient two\n",
    "    # dWhh = <input your code here>\n",
    "    dWhh = np.dot(f_prime, hprev.T)\n",
    "    ## compute the gradient three\n",
    "    # dWhx = <input your code here>\n",
    "    dWhx = np.dot(f_prime, x.T)\n",
    "    return f_prime.T, dWhx, dWhh\n",
    "\n",
    "# compute gradients\n",
    "E = np.sum(h)\n",
    "dEdh = np.ones([hidden_size, 1])\n",
    "\n",
    "dEdhprev, dWhx, dWhh = backward_function(x, hprev, dEdh, Whx, Whh)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Passed! Diff is 0.0\n"
     ]
    }
   ],
   "source": [
    "## Check whether your code is right\n",
    "\n",
    "# Numerical gradient computation\n",
    "epsilon = 1e-7                                         \n",
    "numdWhh = np.zeros([hidden_size,hidden_size])       \n",
    "for i in range(hidden_size):                           \n",
    "    for j in range(hidden_size):                       \n",
    "        newWhh = np.copy(Whh)                          \n",
    "        newWhh[i,j] += epsilon                         \n",
    "                                                           \n",
    "        h = forward_function(x, hprev, Whx, newWhh)\n",
    "        newE = np.sum(h)                               \n",
    "        numdWhh[i,j] = (newE - E) / epsilon            \n",
    "                                                           \n",
    "diff = np.sum(numdWhh - dWhh)                          \n",
    "assert diff < 1e-3                                     \n",
    "print 'Check Passed! Diff is', diff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
